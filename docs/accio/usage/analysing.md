---
layout: accio
nav: accio
title: Analysing results
---

When running experiments, Accio generates a lot of information.
Thankfully, it comes with tools to help analyze all this data.

* TOC
{:toc}

## Output format

All data generated by Accio during a run is dumped inside the directory specified by the `-workdir` argument, or an automatically generated one if none is explicitly specified.
Many experiments can share the same working directory, it is up to you to choose how to organize them.
We describe here the structure of this directory and of the files it contains.

### Metadata

At the root of the working directory, some files are created containing metadata about the experiments that just ran.
Files `experiment-*.json` contain details about the various experiments while files `run-*.json` contain details about the various runs of those experiments.
The file is named after the experiment/run identifier, which is guaranteed to be globally unique.
These files are updated in real-time as the execution of the experiment progresses.

Experiment metadata include a reminder of the workflow being executed, the list of run identifiers being part of this experiment and timing information.

Run metadata include the particular graph being executed, timing information and the output artifacts.

### Artifacts

Some operators produce more than just a scalar value and need to write text or binary content as part of their output.
This data is written under the `data/` directory.
It contains one subdirectory per node, being named after a run identifier and a node name.
For example, `data/7d207929962f18e3e201344f0b7a86ba2df96235-EventSource/` contains the outputs generated by a node named `EventSource` as part of run 7d207929962f18e3e201344f0b7a86ba2df96235.

Operators are then free to use this directory as they want to store their outputs.
A convention is to create one subdirectory per output port and store the actual data inside.

## Exporting data into CSV

Accio command line application includes an `export` command, whose goal is to translate raw information contained inside a working directory into a more readable CSV format.
These CSV files can then be easily digested by many application such as Microsoft Excel and many programming languages.

### Export format

The export generates one CSV file per exported artifact, named after this artifact (slashes being replaced by dashes).
The first line of each CSV is a header line describing the content; it is particularly useful when complex data types are exported, to understand the meaning of each column.
The default behavior is to put write one line per value found in the runs being used.
For example, exporting five runs repeated twice each will generate ten lines in each metric file.
Activating `-split` or `-aggregate` options allow to alter this behavior.

All artifacts can be exported in CSV, although it is more or less useful depending on their type.
For example, exporting a dataset will only print its format and URI.
Lists, sets and maps can also be exported, in which case multiple lines will be generated, one per item they contain.

### Options for `accio export`

This command requires one or many arguments specifying paths to directories containing the output of previous runs.

`-workdir=/path/to/directory`

The working directory is the place where the export will be written.
By default, a random directory will be created in the current directory where Accio is being executed and advertised in the console output, but you can specify a specific directory.
Several export can share the same directory, but in this case be sure to take a look all the options, such as `-append`.

`-separator=";"`

Specifies the separator to use between multiple fields on the same line.
By default it is a single space, but you can specify whichever character you want.
Be careful, no escaping will be done on values.

`-artifacts="PoisRetrieval/recall,PoisRetrieval/precision"`

Specifies a comma-separated list of artifacts to take into account in the export.
You can also use the special `NUMERIC` artifact, which will trigger the inclusion of all artifact whose [type is numeric](model.html). 
By default, only numeric artifacts are included.

`-runs="a12df453,830fh53d"`

Specifies a comma-separated list of runs to take into account in the export.
It can be run or experiment identifiers; in the latter case, all runs of this experiment will be considered.
You can specify only the first characters of each identifier.
By default, all runs found inside the directories specified as arguments are considered.

`-[no]split`
Specifies whether to split the export into multiple directories, depending on workflow parameters.
When this boolean option is activated, each combination of parameters will resulting in a subdirectory named after those parameters, and an export will be generated inside this subdirectory for data correspond to those parameters.
By default, no splitting is performed.

`-[no]aggregate`
Specifies whether to aggregate artifact values across multiple runs into a single value.
When this boolean option is activated, instead of having values for each run accumulated into the output file, values corresponding to the same metric will be merged into a single value.
The way this aggregation is performed depends on the data type, and it may not be possible for all data types.
As of now, the following data types are supported: byte, short, integer, long, double, distance and duration.
Lists and sets of those types are also supported, as well as maps whose values are of those types (keys can be anything, only values are aggregated).
The aggregation operation that is performed when multiple values are found is the average.
By default, no aggregation is performed.

`-[no]append`
Specifies whether data can be appended to existing files if they already exists.
By default, the content of existing files is left untouched.
If there is a conflict between two exports willing to write in the same file (without the `-append` option), an incrementing numeric prefix will be appended to the file name at each execution.